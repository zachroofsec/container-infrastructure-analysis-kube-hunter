nodes:
  - type: Node/Master
    location: 192.168.49.2
services:
  - service: Etcd
    location: 192.168.49.2:2379
  - service: Kubelet API
    location: 192.168.49.2:10250
vulnerabilities:
  - location: 192.168.49.2:10250
    vid: KHV036
    category: Remote Code Execution
    severity: high
    vulnerability: Anonymous Authentication
    description: |-
      The kubelet is misconfigured, potentially allowing secure access to all requests on the kubelet,
          without the need to authenticate
    evidence: |-
      The following containers have been successfully breached.

      Pod namespace: kube-system

      Pod ID: kube-proxy-8ckr8

      Container name: kube-proxy

      Service account token: eyJhbGciOiJSUzI1NiIsImtpZCI6IkM2d0h6SnZiYl9INFdRa2E2QUJPUDY5aVFqLU43UGQ1QjBva2o4QmJDUFEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlLXByb3h5LXRva2VuLW50bnNkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imt1YmUtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiNzRiNzRiYS1hNGVhLTQwOTUtODMzMC1lOWMwODZlODUzNWUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06a3ViZS1wcm94eSJ9.r5FcwQ0bWTeAzqKirfcEblDfJtnf_23vsGoVE2oT8jbyXW6Nw9n5-7FuZkHkWcs29Sd2VKQT-hmdfQ_LH86UyOoPudiMdzDfnyzJh7M7aJlh7Ggk9BaqwJrAQ-iOujkkv-EdSWLk7vVl-n-5I-llfwhhqFu0bjYf3-3IhzNqUK0xJQyVmMtZ6H1ac5m9hA2PlvmMaoUFmGcyATrgwlhTh4SVG9LAwY14hQhyjs6wRJFAtiWJ3zjLX6AwX4J5VAwARJHHgoC9j9BJCPVKxW3nafaWFAk7_dxgaMel23YdecFmlpG99NIE1E3Ho9HcZWb0Q6ukpxhArkW5jlP9MPqjPg
      Environment variables: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      HOSTNAME=minikube
      NODE_NAME=minikube
      KUBE_DNS_PORT_53_UDP=udp://10.96.0.10:53
      KUBE_DNS_PORT_53_UDP_PROTO=udp
      KUBE_DNS_PORT_53_TCP_ADDR=10.96.0.10
      KUBE_DNS_PORT_9153_TCP=tcp://10.96.0.10:9153
      KUBE_DNS_PORT_9153_TCP_PORT=9153
      KUBERNETES_SERVICE_HOST=10.96.0.1
      KUBERNETES_SERVICE_PORT=443
      KUBE_DNS_SERVICE_PORT_DNS=53
      KUBERNETES_PORT_443_TCP_PORT=443
      KUBERNETES_PORT=tcp://10.96.0.1:443
      KUBE_DNS_PORT=udp://10.96.0.10:53
      KUBE_DNS_PORT_9153_TCP_ADDR=10.96.0.10
      KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
      KUBE_DNS_SERVICE_PORT_METRICS=9153
      KUBE_DNS_SERVICE_PORT=53
      KUBE_DNS_PORT_53_TCP=tcp://10.96.0.10:53
      KUBE_DNS_PORT_9153_TCP_PROTO=tcp
      KUBERNETES_SERVICE_PORT_HTTPS=443
      KUBERNETES_PORT_443_TCP_PROTO=tcp
      KUBE_DNS_SERVICE_HOST=10.96.0.10
      KUBE_DNS_PORT_53_UDP_PORT=53
      KUBE_DNS_PORT_53_UDP_ADDR=10.96.0.10
      KUBE_DNS_PORT_53_TCP_PROTO=tcp
      KUBE_DNS_PORT_53_TCP_PORT=53
      KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
      KUBE_DNS_SERVICE_PORT_DNS_TCP=53
      HOME=/root

      Pod namespace: default

      Pod ID: apache-5675f49d78-vt4gq

      Container name: apache

      Service account token: eyJhbGciOiJSUzI1NiIsImtpZCI6IkM2d0h6SnZiYl9INFdRa2E2QUJPUDY5aVFqLU43UGQ1QjBva2o4QmJDUFEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tbmptbTUiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijk1ZDBmMWJlLWIwNjYtNDZlZS04YzdjLWUwYjg0NzNhNjU2MSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.njPsNpT2nDO2ZL9BytOOSDABKKgsXs8hWW9tSAgn4KjiHXepPhjUguzuO4ElLKlOAQA-YCrzKbiWk3PC5dxAhMdwI8mkeNRAXhwYFZcNhPUzS4l_aKZC51RlEzgLfxQe1YoFdj_Xby43T_Y-tvR1a493_wYAmmm16tXEdqi_AhJkkQNrr58GFIYIBK29mzOM5XRGgfhVYx345bXTYjYsFD4yCxXRIWZWmum2u_1oYZMAQqoJsEQ4oKQ2RScYtAQE-ix-ZRFhdkWllwD22xCgfO1fnN5otVGOunuw9pbmcQkqe8JhIaBSRdURmDeO7KDiiSqZAGGmoqBLkc0-3uZnKQ
      Environment variables: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      HOSTNAME=apache-5675f49d78-vt4gq
      APACHE_PORT_443_TCP_PROTO=tcp
      KUBERNETES_SERVICE_HOST=10.96.0.1
      KUBERNETES_SERVICE_PORT=443
      KUBERNETES_PORT=tcp://10.96.0.1:443
      APACHE_SERVICE_HOST=10.96.239.116
      KUBERNETES_SERVICE_PORT_HTTPS=443
      KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
      KUBERNETES_PORT_443_TCP_PROTO=tcp
      KUBERNETES_PORT_443_TCP_PORT=443
      APACHE_PORT_443_TCP=tcp://10.96.239.116:443
      APACHE_PORT_443_TCP_PORT=443
      APACHE_PORT_443_TCP_ADDR=10.96.239.116
      KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
      APACHE_SERVICE_PORT=443
      APACHE_PORT=tcp://10.96.239.116:443
      DEBIAN_FRONTEND=noninteractive
      HOME=/root
    avd_reference: https://avd.aquasec.com/kube-hunter/khv036/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV052
    category: Information Disclosure
    severity: medium
    vulnerability: Exposed Pods
    description: |-
      An attacker could view sensitive information about pods that are
          bound to a Node using the /pods endpoint
    evidence: 'count: 8'
    avd_reference: https://avd.aquasec.com/kube-hunter/khv052/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV043
    category: Information Disclosure
    severity: medium
    vulnerability: Cluster Health Disclosure
    description: |-
      By accessing the open /healthz handler,
          an attacker could get the cluster health state without authenticating
    evidence: 'status: ok'
    avd_reference: https://avd.aquasec.com/kube-hunter/khv043/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV038
    category: Information Disclosure
    severity: medium
    vulnerability: Exposed Running Pods
    description: |-
      Outputs a list of currently running pods,
          and some of their metadata, which can reveal sensitive information
    evidence: 8 running pods
    avd_reference: https://avd.aquasec.com/kube-hunter/khv038/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV046
    category: Information Disclosure
    severity: medium
    vulnerability: Exposed Kubelet Cmdline
    description: Commandline flags that were passed to the kubelet can be obtained from the pprof endpoints
    evidence: "cmdline: /var/lib/minikube/binaries/v1.20.0/kubelet\0--anonymous-auth=True\0--authorization-mode=AlwaysAllow\0--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf\0--config=/var/lib/kubelet/config.yaml\0--container-runtime=docker\0--hostname-override=minikube\0--kubeconfig=/etc/kubernetes/kubelet.conf\0--node-ip=192.168.49.2"
    avd_reference: https://avd.aquasec.com/kube-hunter/khv046/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV037
    category: Information Disclosure
    severity: medium
    vulnerability: Exposed Container Logs
    description: Output logs from a running container are using the exposed /containerLogs endpoint
    evidence: "kube-scheduler: I0219 15:10:48.919991       1 serving.go:331] Generated self-signed cert in-memory\nW0219 15:10:52.405835       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'\nW0219 15:10:52.406182       1 authentication.go:332] Error looking up in-cluster authentication configuration: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\nW0219 15:10:52.406327       1 authentication.go:333] Continuing without authentication configuration. This may treat all requests as anonymous.\nW0219 15:10:52.406395       1 authentication.go:334] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false\nI0219 15:10:52.442379       1 secure_serving.go:197] Serving securely on 127.0.0.1:10259\nE0219 15:10:52.451351       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope\nE0219 15:10:52.451443       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope\nE0219 15:10:52.451488       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope\nE0219 15:10:52.452333       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope\nI0219 15:10:52.459087       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI0219 15:10:52.459260       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI0219 15:10:52.459354       1 tlsconfig.go:240] Starting DynamicServingCertificateController\nE0219 15:10:52.460042       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope\nE0219 15:10:52.462335       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope\nE0219 15:10:52.465266       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope\nE0219 15:10:52.466381       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope\nE0219 15:10:52.466463       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope\nE0219 15:10:52.476171       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope\nE0219 15:10:52.476351       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\nE0219 15:10:52.476446       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.PodDisruptionBudget: failed to list *v1beta1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope\nE0219 15:10:53.281329       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope\nE0219 15:10:53.307749       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope\nE0219 15:10:53.389787       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope\nE0219 15:10:53.850474       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\nI0219 15:10:55.660005       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file \n"
    avd_reference: https://avd.aquasec.com/kube-hunter/khv037/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV040
    category: Remote Code Execution
    severity: high
    vulnerability: Exposed Run Inside Container
    description: An attacker could run an arbitrary command inside a container
    evidence: |
      uname -a: Linux minikube 5.7.0-kali1-amd64 #1 SMP Debian 5.7.6-1kali2 (2020-07-01) x86_64 GNU/Linux
    avd_reference: https://avd.aquasec.com/kube-hunter/khv040/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV045
    category: Information Disclosure
    severity: medium
    vulnerability: Exposed System Logs
    description: System logs are exposed from the /logs endpoint on the kubelet
    evidence: Could not parse system logs
    avd_reference: https://avd.aquasec.com/kube-hunter/khv045/
    hunter: Kubelet Secure Ports Hunter
  - location: 192.168.49.2:10250
    vid: KHV051
    category: Access Risk
    severity: low
    vulnerability: Exposed Existing Privileged Container(s) Via Secure Kubelet Port
    description: A malicious actor, that has confirmed anonymous access to the API via the kubelet's secure port, can leverage the existing privileged containers identified to damage the host and potentially the whole cluster
    evidence: The following exposed existing privileged containers were not successfully abused by starting/modifying a process in the host.Keep in mind that attackers might use other methods to attempt to abuse them.
    avd_reference: https://avd.aquasec.com/kube-hunter/khv051/
    hunter: Foothold Via Secure Kubelet Port
